{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "51e7d114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc9849e",
   "metadata": {},
   "source": [
    "### Reshape a vector \n",
    "\n",
    "````python\n",
    "x.reshape((n, m))\n",
    "````\n",
    "- n lignes et m colonnes\n",
    "- si on a x.reshape((-1, m)) ou x.reshape((n, -1))\n",
    "    - La fonction redrimmensionnera automatiquent le nombre de ligne ou de le colonne de manière à avoir m colonnes ou n lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a08e8024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(x):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    x -- a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:\n",
    "    v -- a vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    a = x.shape[0]\n",
    "    b = x.shape[1]\n",
    "    c = x.shape[2]\n",
    "    \n",
    "    v = x.reshape((a * b * c, 1))\n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "97963e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2vector(image) = [[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "t_image = np.array([[[ 0.67826139,  0.29380381],\n",
    "                     [ 0.90714982,  0.52835647],\n",
    "                     [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "                   [[ 0.92814219,  0.96677647],\n",
    "                    [ 0.85304703,  0.52351845],\n",
    "                    [ 0.19981397,  0.27417313]],\n",
    "\n",
    "                   [[ 0.60659855,  0.00533165],\n",
    "                    [ 0.10820313,  0.49978937],\n",
    "                    [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print (\"image2vector(image) = \" + str(reshape(t_image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360f7578",
   "metadata": {},
   "source": [
    "### np.multiply\n",
    "\n",
    "Multiplication élément par élément sur des tableaux.\n",
    "\n",
    "- x1, x2 :\n",
    "    - Type : array_like\n",
    "    - Description : Ce sont les tableaux (ou éléments) à multiplier. x1 et x2 doivent être de formes compatibles pour la multiplication élément par élément. S'ils n'ont pas la même forme, NumPy tente de les diffuser (broadcast) pour qu'ils aient des formes compatibles.\n",
    "\n",
    "- out (facultatif) :\n",
    "    - Type : ndarray, None, ou tuple de ndarrays et None, par défaut est None.\n",
    "    - Description : Un emplacement alternatif où le résultat de la multiplication est stocké. S'il est fourni, il doit avoir une forme qui peut accueillir le résultat. S'il n'est pas fourni, un nouveau tableau est créé pour le résultat.\n",
    "\n",
    "- where (facultatif) :\n",
    "    - Type : array_like de bool, par défaut est True.\n",
    "    - Description : Cet argument est un tableau conditionnel de type booléen. Lorsqu'il est utilisé, la multiplication est effectuée et le résultat est stocké uniquement aux emplacements où le tableau conditionnel a une valeur True. Cela permet de faire une multiplication conditionnelle.\n",
    "\n",
    "- casting (facultatif) :\n",
    "    - Type : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, par défaut 'same_kind'.\n",
    "    - Description : Contrôle le type de casting autorisé pour les opérations. Il s'agit d'une précaution pour s'assurer que les types sont compatibles pour la multiplication.\n",
    "\n",
    "- order (facultatif) :\n",
    "    - Type : {'C', 'F', 'A', 'K'}, par défaut 'K'.\n",
    "    - Description : Spécifie l'ordre de mémoire du tableau résultant. 'C' pour l'ordre C-major, 'F' pour Fortran-major (colonne-major), 'A' signifie 'F' si x1 est Fortran-contigu, 'C' sinon, et 'K' signifie aussi contigu que possible.\n",
    "\n",
    "- dtype (facultatif) :\n",
    "    - Type : dtype, par défaut None.\n",
    "    - Description : Le type de données désiré pour le tableau résultant. Si None, le type de données est déterminé automatiquement.\n",
    "\n",
    "- subok (facultatif) :\n",
    "    - Type : bool, par défaut True.\n",
    "    - Description : Si True, alors les sous-classes seront passées telles quelles, sinon la nouvelle instance de tableau sera retournée.\n",
    "\n",
    "- signature (facultatif) :\n",
    "    - Type : string ou None, par défaut None.\n",
    "    - Description : Vous pouvez spécifier une signature explicite pour la fonction multiply. Ceci est utile pour des cas d'utilisation avancés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "020506c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  5, 18])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Création de deux tableaux pour la multiplication\n",
    "x1 = np.array([1, 2, 3])\n",
    "x2 = np.array([4, 5, 6])\n",
    "\n",
    "# Emplacement alternatif pour le résultat\n",
    "out = np.empty_like(x1)\n",
    "\n",
    "# Tableau conditionnel pour where\n",
    "where = np.array([True, False, True])\n",
    "\n",
    "# Exemple d'utilisation de np.multiply avec tous les arguments\n",
    "result = np.multiply(\n",
    "    x1, x2,\n",
    "    out=out,\n",
    "    where=where,\n",
    "    casting='same_kind',\n",
    "    order='K',\n",
    "    dtype=None,\n",
    "    subok=True\n",
    ")\n",
    "\n",
    "result\n",
    "### array([      4, 9458688,      18]) # --> 2*5 = 9458688 or stg else car False dans where"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b825e4e1",
   "metadata": {},
   "source": [
    "### Trick to reshape an image\n",
    "\n",
    "A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b$*$c$*$d, a) is to use: \n",
    "```python\n",
    "X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "69894ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim de m :(209, 64, 64, 3)\n",
      "nombre d'exemple : 209\n",
      "nombre de pixel : 64\n",
      "nombre de pixel totaux (64, 64, 3)\n",
      "dimension de m applatie : (12288, 209)\n",
      "dimension de x : (64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "m = np.random.rand(209, 64, 64, 3) * 255 # Compris entre 0 et 255\n",
    "a = m.shape[0]\n",
    "px = m[25].shape[0]\n",
    "pxt = m[25].shape\n",
    "m_flattened = m.reshape(a, -1).T\n",
    "\n",
    "dim = m_flattened.shape\n",
    "\n",
    "# Recirpocal reshape\n",
    "index = 25\n",
    "x1 = m_flattened[:, index].reshape((px, px, 3))\n",
    "\n",
    "print(\"dim de m :\" + str(m.shape))\n",
    "print(\"nombre d'exemple : \" + str(a))\n",
    "print(\"nombre de pixel : \" + str(px))\n",
    "print(\"nombre de pixel totaux \" + str(pxt))\n",
    "print(\"dimension de m applatie : \" + str(dim))\n",
    "print(\"dimension de x : \" + str(x1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2e3bb77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(m_flattened[0, :], m_flattened[1, :]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb5dad",
   "metadata": {},
   "source": [
    "### Propagate\n",
    "\n",
    "- You get X\n",
    "- You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))$\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2adcda3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    grads -- dictionary containing the gradients of the weights and bias\n",
    "            (dw -- gradient of the loss with respect to w, thus same shape as w)\n",
    "            (db -- gradient of the loss with respect to b, thus same shape as b)\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    z = np.dot(w.T, X) + b\n",
    "    A = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    cost = -np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A) ) / m\n",
    "\n",
    "    dw = np.dot(X, (A - Y).T) / m\n",
    "    db = np.sum(A - Y) / m\n",
    "    \n",
    "    cost = np.squeeze(np.array(cost))\n",
    "\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fb72bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    w = copy.deepcopy(w)\n",
    "    b = copy.deepcopy(b)\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 10000 == 0:\n",
    "            costs.append(cost)\n",
    "            # Print the cost every 100 training iterations\n",
    "            if print_cost:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a02034f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((m))\n",
    "    w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    z = np.dot(w.T, X) + b\n",
    "    A = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    for i in range (m):\n",
    "        if A[0, i] > 0.5 :\n",
    "            Y_prediction[i] = 1\n",
    "        else :\n",
    "            Y_prediction[i] = 0\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0d3304b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[-3.59140566]\n",
      " [ 5.41372937]]\n",
      "b = 1.4944176057118204\n",
      "dw = [[ 3.51685885e-06]\n",
      " [-4.88853112e-06]]\n",
      "db = -1.4702535557506077e-06\n",
      "Costs = [array(0.54132348), array(0.00033018), array(0.00016675), array(0.00011181), array(8.41848328e-05), array(6.75484715e-05), array(5.64242266e-05), array(4.84588424e-05), array(4.24724604e-05), array(3.78080973e-05), array(3.40707846e-05), array(3.10087442e-05), array(2.84538658e-05), array(2.62896049e-05), array(2.44326106e-05), array(2.28216892e-05), array(2.14108967e-05), array(2.01650652e-05), array(1.90568171e-05), array(1.806452e-05), array(1.71708543e-05), array(1.63617888e-05), array(1.56258352e-05), array(1.4953497e-05), array(1.43368552e-05), array(1.37692539e-05), array(1.32450581e-05), array(1.27594647e-05), array(1.23083548e-05), array(1.18881756e-05), array(1.14958468e-05), array(1.11286841e-05), array(1.07843377e-05), array(1.04607419e-05), array(1.0156073e-05), array(9.86871535e-06), array(9.59723204e-06), array(9.34034113e-06), array(9.09689518e-06), array(8.86586406e-06), array(8.64632035e-06), array(8.43742683e-06), array(8.23842577e-06), array(8.0486297e-06), array(7.86741348e-06), array(7.69420733e-06), array(7.52849091e-06), array(7.369788e-06), array(7.21766196e-06), array(7.07171167e-06), array(6.93156802e-06), array(6.79689075e-06), array(6.66736571e-06), array(6.54270237e-06), array(6.42263167e-06), array(6.30690406e-06), array(6.19528775e-06), array(6.08756718e-06), array(5.9835416e-06), array(5.88302385e-06), array(5.78583919e-06), array(5.69182431e-06), array(5.6008264e-06), array(5.51270236e-06), array(5.42731795e-06), array(5.3445472e-06), array(5.26427176e-06), array(5.18638031e-06), array(5.11076806e-06), array(5.03733627e-06), array(4.96599185e-06), array(4.89664692e-06), array(4.82921847e-06), array(4.76362803e-06), array(4.69980136e-06), array(4.63766816e-06), array(4.57716183e-06), array(4.51821922e-06), array(4.46078039e-06), array(4.40478846e-06), array(4.35018934e-06), array(4.29693166e-06), array(4.2449665e-06), array(4.19424731e-06), array(4.14472976e-06), array(4.09637159e-06), array(4.04913249e-06), array(4.00297401e-06), array(3.95785946e-06), array(3.91375375e-06), array(3.87062339e-06), array(3.82843633e-06), array(3.78716192e-06), array(3.74677082e-06), array(3.70723495e-06), array(3.66852738e-06), array(3.63062234e-06), array(3.59349507e-06), array(3.55712186e-06), array(3.52147994e-06)]\n",
      "predictions = [1. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "w =  np.array([[0.1124579], [0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1., -2., -1., 0, 2.], [3., 0.5, -3.2, 2.5, -1.8]])\n",
    "Y = np.array([[1, 1, 0, 1, 0]])\n",
    "params, grads, costs = optimize(w, b, X, Y, num_iterations=1000000, learning_rate=0.09, print_cost=False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print(\"Costs = \" + str(costs))\n",
    "\n",
    "#w = np.array([[0.1124579], [0.23106775]])\n",
    "#b = -0.3\n",
    "#X = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84238fe0",
   "metadata": {},
   "source": [
    "### Logistic regression with scikit learn\n",
    "\n",
    "````python \n",
    "clf = sklearn.linear_model.LogisticRegressionCV();\n",
    "\n",
    "clf.fit(X.T, Y.T);\n",
    "\n",
    "LR_predictions = clf.predict(X.T)\n",
    "\n",
    "# Plot the decision boundary for logistic regression\n",
    "plot_decision_boundary(lambda x: clf.predict(x), X, Y)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a116e3",
   "metadata": {},
   "source": [
    "### Defining the neural network structure\n",
    "\n",
    "Là il s'agit de la construction d'un modèle non deep, pour un modèle deep il y aurra des boucles for.\n",
    "\n",
    "\n",
    "1. **layer sizes**\n",
    "\n",
    "\n",
    "2. **initialize parameters**\n",
    "    - n_x, n_hiden layer, n_y\n",
    "    - -->    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    \n",
    "3. **forward propagation**\n",
    "    - X, parameters\n",
    "    - --> A2, cache= {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    \n",
    "    \n",
    "4. **cost**\n",
    "\n",
    "\n",
    "5. **Backward propagation**\n",
    "    - parameters, cache, X, Y\n",
    "    - grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "\n",
    "6. **update parameters** \n",
    "    - parameters, grads, learning_rate\n",
    "    - parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "\n",
    "\n",
    "7. **Neural Network**\n",
    "    - X, Y, n_h, num_iterations\n",
    "    - func : \n",
    "        - initialize_parameters\n",
    "        - forward_propagation\n",
    "        - compute_cost\n",
    "        - backward_propagation\n",
    "        - update_parameters\n",
    "    - parameters\n",
    "    \n",
    "    \n",
    "8. **prediction**\n",
    "    - parameters, X\n",
    "    - func : forward_propagation\n",
    "    - predictions = (A2 > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8dc138a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: layer_sizes\n",
    "\n",
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    n_x = X.shape[0]\n",
    "    n_h = 4\n",
    "    n_y = Y.shape[0]\n",
    "\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "372fa9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"    \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4ae2909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = 1 / (1 + np.exp(-Z2))\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "481c75ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of examples\n",
    "\n",
    "    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1 - Y))\n",
    "    cost = - np.sum(logprobs) / m\n",
    "    \n",
    "    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n",
    "                                    # E.g., turns [[17]] into 17 \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fac7f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(dZ2, A1.T) / m\n",
    "    db2 = np.sum(dZ2, axis = 1, keepdims = True) / m\n",
    "    dZ1 = W2.T * dZ2 * (1 - np.power(A1, 2))\n",
    "    dW1 = np.dot(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis = 1, keepdims = True) / m\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "312015fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    param = copy.deepcopy(parameters)\n",
    "    W1 = param[\"W1\"]\n",
    "    b1 = param[\"b1\"]\n",
    "    W2 = param[\"W2\"]\n",
    "    b2 = param[\"b2\"]\n",
    "\n",
    "    grads = copy.deepcopy(grads)\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9d2bce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(A2, Y)\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate = 1.2)\n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2d09c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A2 > 0.5)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c6afe8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[ True False  True]]\n"
     ]
    }
   ],
   "source": [
    "def predict_test_case():\n",
    "    np.random.seed(1)\n",
    "    X_assess = np.random.randn(2, 3)\n",
    "    parameters = {'W1': np.array([[-0.00615039,  0.0169021 ],\n",
    "        [-0.02311792,  0.03137121],\n",
    "        [-0.0169217 , -0.01752545],\n",
    "        [ 0.00935436, -0.05018221]]),\n",
    "     'W2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),\n",
    "     'b1': np.array([[ -8.97523455e-07],\n",
    "        [  8.15562092e-06],\n",
    "        [  6.04810633e-07],\n",
    "        [ -2.54560700e-06]]),\n",
    "     'b2': np.array([[  9.14954378e-05]])}\n",
    "    return parameters, X_assess\n",
    "\n",
    "parameters, t_X = predict_test_case()\n",
    "\n",
    "predictions = predict(parameters, t_X)\n",
    "print(\"Predictions: \" + str(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7f2e19c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693148\n",
      "Cost after iteration 1000: 0.002404\n",
      "Cost after iteration 2000: 0.000881\n",
      "Cost after iteration 3000: 0.000490\n",
      "Cost after iteration 4000: 0.000332\n",
      "Cost after iteration 5000: 0.000249\n",
      "Cost after iteration 6000: 0.000199\n",
      "Cost after iteration 7000: 0.000165\n",
      "Cost after iteration 8000: 0.000140\n",
      "Cost after iteration 9000: 0.000122\n",
      "W1 = [[-2.59291847  5.09419202]\n",
      " [-1.91781073 -0.88519898]\n",
      " [-1.11925713 -6.44401006]\n",
      " [-1.93370294 -1.09339302]]\n",
      "b1 = [[ 1.67883652]\n",
      " [-0.7256764 ]\n",
      " [ 0.52277942]\n",
      " [-0.82311107]]\n",
      "W2 = [[-7.40704158  3.38656101 -7.63078975  2.57030995]]\n",
      "b2 = [[5.38209184]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.randn(2, 4)\n",
    "Y = (np.random.randn(1, 4) > 0)\n",
    "n_h = 4\n",
    "\n",
    "t_X = np.random.randn(2, 4)\n",
    "t_Y = (np.random.randn(1, 4) > 0)\n",
    "\n",
    "parameters = nn_model(t_X, t_Y, n_h, num_iterations=10000, print_cost=True)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd434b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a21d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba258506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b502be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3d60d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b5f49f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b234ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1030b5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d8a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa8325e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be5dafa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d2c684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69db8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b64fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aac7381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e671d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac088413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97de1f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a258079d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc851389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c2916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66a1b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e5651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de93ccac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c429cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d649c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc3255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae71b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da6594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9225535e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8349ea2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a67e646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52272f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
